{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Bunch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-fccd7f87f6e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdatetime\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0myellowbrick\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregressor\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mResidualsPlot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/arjunrao/anaconda/lib/python3.6/site-packages/yellowbrick/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;31m# Import yellowbrick functionality to the top level\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;31m# TODO: review top-level functionality\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0manscombe\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0manscombe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mclassifier\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mROCAUC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mClassBalance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mClassificationScoreVisualizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;31m# from .classifier import crplot, rocplot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/arjunrao/anaconda/lib/python3.6/site-packages/yellowbrick/anscombe.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0myellowbrick\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbestfit\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdraw_best_fit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0myellowbrick\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstyle\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_color_cycle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/arjunrao/anaconda/lib/python3.6/site-packages/yellowbrick/bestfit.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlinear_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPolynomialFeatures\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipeline\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmake_pipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmean_squared_error\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/arjunrao/anaconda/lib/python3.6/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mexternals\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetaestimators\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mif_delegate_has_method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBunch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcheck_memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'Bunch'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import statsmodels.tsa.stattools as ts\n",
    "from statsmodels.tsa.stattools import acf, pacf\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "\n",
    "from pandas.tools.plotting import autocorrelation_plot\n",
    "from pandas.plotting import lag_plot\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from datetime import date\n",
    "import seaborn as sb\n",
    "from yellowbrick.regressor import ResidualsPlot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_plot(title, width=10, height=7):\n",
    "    plt.figure(figsize=(width, height))\n",
    "    plt.title(title, fontsize=15)\n",
    "     \n",
    "def plot_residuals(model, x_train, x_test, y_train, y_test):\n",
    "    visualizer = ResidualsPlot(model)\n",
    "\n",
    "    visualizer.fit(x_train, y_train)  # Fit the training data to the visualizer\n",
    "    visualizer.score(x_test, y_test)  # Evaluate the model on the test data\n",
    "    g = visualizer.poof() \n",
    "    \n",
    "def adf_test(y):\n",
    "    '''\n",
    "    Perform Augmented Dickey Fuller test, for stationarity.\n",
    "    '''\n",
    "    print('Results of Augmented Dickey-Fuller test:')\n",
    "    dftest = adfuller(y, autolag='AIC')\n",
    "    dfoutput = pd.Series(dftest[0:4], index=['test statistic', 'p-value', '# of lags', '# of observations'])\n",
    "    for key, value in dftest[4].items():\n",
    "        dfoutput['Critical Value ({})'.format(key)] = value\n",
    "    print(dfoutput)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Data\n",
    "    We read in the dataset as a time series, and drop outliers in our dataset. \n",
    "    \n",
    "    We also make both time series stationary. It is important to impose the requirement of stationarity because a stationary process has the property that the mean, variance and autocorrelation structure do not change over time. Then, it is relatively easy to predict: One can predict that its statistical properties will be the same in the future as they have been in the past."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ Read and Process Data ------------------ #\n",
    "\n",
    "# I am replacing outliers instead of dropping for sake of window\n",
    "df = pd.read_csv(\"stockdata.csv\", index_col='date', parse_dates=['date'])\n",
    "\n",
    "# Remove outliers via dropping\n",
    "sig_val = df['signal'].value_counts()\n",
    "criteria = df[ (df['signal'] <= 2) | (df['signal'] >= 400) ]\n",
    "df['signal'][criteria.index] = np.nan\n",
    "\n",
    "sig_val = df['spy_close_price'].value_counts()\n",
    "criteria = df[ df['spy_close_price'] >= 500 ]\n",
    "df['spy_close_price'][criteria.index] = np.nan\n",
    "\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# ------------- Make Time Series Stationary ------------- #\n",
    "# We would like to make the series stationary, for many properties\n",
    "# that hold for independent variables also hold for stationary ones.\n",
    "\n",
    "# sliding window size to simulate 2 weeks ( 10 days )\n",
    "window = 10\n",
    "\n",
    "# Log transformation\n",
    "sig_log = np.log(df['signal'])\n",
    "price_log = np.log(df['spy_close_price'])\n",
    "\n",
    "# Expotential Weighted Moving average for log transformed:\n",
    "# Weights are assigned to all the previous values with a decay factor. \n",
    "sig_expw_avg = sig_log.ewm(halflife=window, min_periods=0, adjust=True, ignore_na=False).mean()\n",
    "price_expw_avg = price_log.ewm(halflife=window, min_periods=0, adjust=True, ignore_na=False).mean()\n",
    "\n",
    "sig_ewma_diff = sig_log - sig_expw_avg\n",
    "price_ewma_diff = price_log - price_expw_avg\n",
    "\n",
    "# The stationary series.\n",
    "sig_dif = sig_ewma_diff\n",
    "price_dif = price_ewma_diff\n",
    "\n",
    "# Confirm the stationarity. We reject the null hypothesis\n",
    "# that there is a unit root (nonstationary variable)\n",
    "sad = adfuller(sig_dif)\n",
    "pad = adfuller(price_dif)\n",
    "print('Augmented Dickey-Fuller test:')\n",
    "print('Critical values: ', sad[4])\n",
    "print('signal test statistic: ', sad[0])\n",
    "print('stockprice test statistic: ', pad[0])\n",
    "\n",
    "# Data frame with stationary series\n",
    "df_data = pd.concat([sig_dif, price_dif], axis=1)\n",
    "df_data.dropna(inplace=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression\n",
    "    Linear regression attempts to model the relationship between two variables by fitting a linear equation to observed data. One variable is considered to be an explanatory variable, and the other is considered to be a dependent variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-b505e9f08b99>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Use 75% of data as training, and 25% for test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdiv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m0.75\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# No need for stationarity here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'signal'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# Use 75% of data as training, and 25% for test\n",
    "div = int(len(df)*0.75)\n",
    "\n",
    "# No need for stationarity here\n",
    "X = df[['signal']]\n",
    "Y = df[['spy_close_price']]\n",
    "X_train, X_test , y_train , y_test = train_test_split(X,Y,test_size=0.25,random_state=34)\n",
    "\n",
    "# Train the model and predict for x_test\n",
    "lm = LinearRegression()\n",
    "lm.fit(X_train, y_train)\n",
    "predicted = lm.predict(X_test)\n",
    "expected = y_test\n",
    "\n",
    "# The coefficients\n",
    "print('Coefficients: \\n', lm.coef_)\n",
    "# The mean squared error\n",
    "print(\"Mean squared error: %.7f\"\n",
    "      % mean_squared_error(y_true=expected, y_pred=predicted))\n",
    "# Explained variance score: 1 is perfect prediction\n",
    "print('R-squared = : %.7f' % r2_score(y_true=expected, y_pred=predicted))\n",
    "\n",
    "setup_plot(\"Simple Regression of Price on Signal:\")\n",
    "plt.scatter(X_test, y_test)\n",
    "plt.plot(X_test, predicted, color='pink')\n",
    "\n",
    "plt.xlabel('Signal', fontsize=18)\n",
    "plt.ylabel('Spy Close Price', fontsize=18)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residuals\n",
    "\n",
    "residuals = pd.DataFrame(expected-predicted)\n",
    "residuals.plot()\n",
    "plt.show()\n",
    "\n",
    "residuals.plot(kind='kde')\n",
    "plt.show()\n",
    "print(residuals.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    We seek a random distributed residual plot. \n",
    "    \n",
    "    From the above residual description, the mean = ~0.0. \n",
    "    A non zero mean indicates bias in the prediction, and trend\n",
    "    that has not been captured by the model.\n",
    "\n",
    "    The mean score above indicates our current model captures \n",
    "    the trend from X (signal)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forecasting spy_close_price via Autoregression\n",
    "https://machinelearningmastery.com/autoregression-models-time-series-forecasting-python/\n",
    "\n",
    "    Autoregression is a time series model that uses observations from previous time steps as input to a regression equation to predict the value at the next time step. It models an output value based on a linear combination of input values, where the input variables are observations at previous steps. Autocorrelation is the assumption that variables at previous steps are userful for predicting the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, to verify that autocorrelation exits in stock price.\n",
    "# The clustering in the graph indicates autocorrelation does.\n",
    "setup_plot(\"Lag Plot of spy_close_price\")\n",
    "lag_plot(df_data['spy_close_price'], lag = window)\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dataset, test_start=index where test starts\n",
    "# Note: we are using the stationary data\n",
    "X = df_data['spy_close_price']\n",
    "test_start = 500\n",
    "train, test = X[1:test_start], X[test_start:]\n",
    "test_index = X[test_start:].index\n",
    "\n",
    "# train autoregression\n",
    "model = AR(train)\n",
    "model_fit = model.fit()\n",
    "window = model_fit.k_ar # lag\n",
    "coef = model_fit.params\n",
    "\n",
    "# walk forward over time steps in test aka start.\n",
    "# predicting for X[test_start:] using the steps=window behind it.\n",
    "\n",
    "# Use the learned coefficients and manually make predictions. \n",
    "# This requires that the history of 29 prior observations be kept \n",
    "# and that the coefficients be retrieved from the model and used \n",
    "# in the regression equation to come up with new forecasts.\n",
    "\n",
    "history = train[test_start-window:]\n",
    "history = [history[i] for i in range(len(history))]\n",
    "predictions = list()\n",
    "\n",
    "for t in range(len(test)):\n",
    "    length = len(history)\n",
    "    lag = [history[i] for i in range(length-window,length)]\n",
    "    yhat = coef[0]\n",
    "    for d in range(window):\n",
    "        yhat += coef[d+1] * lag[window-d-1]\n",
    "    obs = test[t]\n",
    "    predictions.append(yhat)\n",
    "    history.append(obs)\n",
    "    # print('predicted=%f, expected=%f' % (yhat, obs))\n",
    "\n",
    "\n",
    "print('\\nOptimal Lag: %.2f' % model_fit.k_ar)\n",
    "\n",
    "error = mean_squared_error(test, predictions)\n",
    "print('\\nTest MSE: %.7f' % error)\n",
    "\n",
    "# Make a dataframe out of the predictions, for plotting purposes\n",
    "df_pred = pd.DataFrame(index=test_index, \n",
    "                       data=predictions, \n",
    "                       columns=['price_prediction'])\n",
    "\n",
    "setup_plot(\"AutoRegression (Predicting test from train)\")\n",
    "plt.plot(test, label='Expected')\n",
    "plt.plot(df_pred['price_prediction'], color='pink', label='Predicted')\n",
    "plt.legend()\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Note the lag value from training the data is 18. \n",
    "    18 is roughly ~ 4 weeks of previous stocks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think my linear regression one is wrong,\n",
    "why is the correlation so high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
